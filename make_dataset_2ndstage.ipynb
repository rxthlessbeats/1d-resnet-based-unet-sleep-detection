{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "from contextlib import contextmanager\n",
    "\n",
    "def show_memory_usage(name = \"unknown\"):\n",
    "    vm = psutil.virtual_memory()\n",
    "    print(f\"[MEMUSE] memory usage (in {name}): {vm.used/1024/1024:.2f}MB ({vm.percent}%)\")\n",
    "\n",
    "@contextmanager\n",
    "def timer(name: str):\n",
    "    show_memory_usage(f\"before {name}\")\n",
    "    s = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - s\n",
    "    print(f\"[{name}] {elapsed:.3f}sec\")\n",
    "    show_memory_usage(f\"after {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.steps_per_sec = 0.2\n",
    "        self.step_for_a_day = 60 * self.steps_per_sec * 60 * 24\n",
    "        self.step_for_30min = 60 * self.steps_per_sec * 30\n",
    "        self.step_for_15min = 60 * self.steps_per_sec * 15\n",
    "        self.step_for_1min = 60 * self.steps_per_sec\n",
    "\n",
    "    def from_json(self, json_path):\n",
    "        json_data = json.load(open(json_path))\n",
    "        for k, v in json_data.items():\n",
    "            print(k, v)\n",
    "            setattr(self, k, v)\n",
    "        return self\n",
    "\n",
    "setting_file = \"SETTINGS.json\"\n",
    "Cfg = Config().from_json(setting_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "cimport numpy as cnp\n",
    "cimport cython\n",
    "\n",
    "def cumsum_morethan_zero(cnp.ndarray[cnp.float64_t, ndim=1] x):\n",
    "    cdef int i, n\n",
    "    n = x.shape[0]\n",
    "    cdef cnp.ndarray[cnp.float64_t, ndim=1] y = np.zeros(n)\n",
    "    cdef cnp.ndarray[cnp.float64_t, ndim=1] y_rev = np.zeros(n)\n",
    "    y[0] = x[0]\n",
    "    for i in range(1, n):\n",
    "        if x[i] == 0:\n",
    "            y[i] = 0\n",
    "        else:\n",
    "            y[i] = y[i-1] + x[i]\n",
    "    y_rev[-1] = y[-1]\n",
    "    for i in range(n-2, -1, -1):\n",
    "        if y_rev[i+1] > y[i]:\n",
    "            if x[i] == 0:\n",
    "                y_rev[i] = 0\n",
    "            else:\n",
    "                y_rev[i] = y_rev[i+1]\n",
    "        else:\n",
    "            y_rev[i] = y[i]\n",
    "    return y_rev\n",
    "\n",
    "def easy_convolve(cnp.ndarray[cnp.float64_t, ndim=1] x, int filter_size):\n",
    "    \"\"\"\n",
    "    padding same, kernel is ones\n",
    "    \"\"\"\n",
    "    cdef int i, j, n, p, m\n",
    "    m = filter_size - 1\n",
    "    p = m // 2\n",
    "    n = x.shape[0]\n",
    "    cdef cnp.ndarray[cnp.float64_t, ndim=1] x_p = np.zeros(n+2*p)\n",
    "    cdef cnp.ndarray[cnp.float64_t, ndim=1] y = np.zeros(n)\n",
    "    x_p[p:n+p] = x\n",
    "\n",
    "    for j in range(filter_size):\n",
    "        y[0] += x_p[j]\n",
    "\n",
    "    for i in range(1, n):# filter_size, n+p+p-filter_size+1):\n",
    "        y[i] = x_p[i+m] + y[i-1] - x_p[i-1]\n",
    "    return y\n",
    "\n",
    "def minimum(cnp.ndarray[cnp.float64_t, ndim=1] x, cnp.float64_t maxval):\n",
    "    cdef int i, n\n",
    "    n = x.shape[0]\n",
    "    cdef cnp.ndarray[cnp.float64_t, ndim=1] y = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        y[i] = min(x[i], maxval)\n",
    "    return y\n",
    "\n",
    "def easy_closing(cnp.ndarray[cnp.float64_t, ndim=1] x, int filter_size):\n",
    "    \"\"\"\n",
    "    closing = dilation -> erosion\n",
    "    padding same, kernel is ones, x is 0 or 1\n",
    "    \"\"\"\n",
    "    x = easy_convolve(x, filter_size)\n",
    "    x = minimum(x, 1)\n",
    "    x = 1 - x\n",
    "    x = easy_convolve(x, filter_size)\n",
    "    x = minimum(x, 1)\n",
    "    x = 1 - x\n",
    "    return x\n",
    "\n",
    "def easy_closing_q(cnp.ndarray[cnp.float64_t, ndim=1] x, int filter_size):\n",
    "    \"\"\"\n",
    "    closing = dilation -> erosion\n",
    "    padding same, kernel is ones, x is 0 or 1\n",
    "    少し早いけどわかりにくい…。\n",
    "    \"\"\"\n",
    "    cdef int i, j, n, p, m\n",
    "    m = filter_size - 1\n",
    "    p = m // 2\n",
    "    n = x.shape[0]\n",
    "    cdef cnp.ndarray[cnp.float64_t, ndim=1] x_p = np.zeros(n+2*p)\n",
    "    cdef cnp.ndarray[cnp.float64_t, ndim=1] y_p = np.zeros(n+2*p)\n",
    "    cdef cnp.ndarray[cnp.float64_t, ndim=1] y = np.zeros(n)\n",
    "    cdef cnp.ndarray[cnp.float64_t, ndim=1] z = np.zeros(n)\n",
    "    \n",
    "    x_p[p:n+p] = x\n",
    "    for j in range(filter_size):\n",
    "        y[0] += x_p[j]\n",
    "    for i in range(1, n):# filter_size, n+p+p-filter_size+1):\n",
    "        y[i] = x_p[i+m] + y[i-1] - x_p[i-1]\n",
    "    for i in range(n):\n",
    "        y[i] = 1 - min(y[i], 1)\n",
    "    \n",
    "    y_p[p:n+p] = y\n",
    "    for j in range(filter_size):\n",
    "        z[0] += y_p[j]\n",
    "    for i in range(1, n):# filter_size, n+p+p-filter_size+1):\n",
    "        z[i] = y_p[i+m] + z[i-1] - y_p[i-1]\n",
    "    for i in range(n):\n",
    "        z[i] = 1 - min(z[i], 1)\n",
    "    \n",
    "    return z\n",
    "\n",
    "\n",
    "def _detect_peak(cnp.ndarray[cnp.float64_t, ndim=1] x, int k):\n",
    "    cdef int n = x.shape[0]\n",
    "    cdef cnp.ndarray[cnp.float64_t, ndim=1] max_array = np.zeros(n, dtype=np.float64)\n",
    "    cdef cnp.ndarray[cnp.int32_t, ndim=1] max_indices = np.zeros(n, dtype=np.int32)\n",
    "    cdef cnp.ndarray[cnp.float64_t, ndim=1] result = np.zeros(n, dtype=np.float64)\n",
    "    cdef int i, j, start, end, max_index\n",
    "    \n",
    "    # calculate max values in each window\n",
    "    for i in range(n):\n",
    "        start = max(0, i - k)\n",
    "        end = min(n, i + k + 1)\n",
    "        max_index = start\n",
    "        for j in range(start, end):\n",
    "            if x[j] > x[max_index]:\n",
    "                max_index = j\n",
    "        max_array[i] = x[max_index]\n",
    "        max_indices[i] = max_index\n",
    "    \n",
    "    # set peak values to 1\n",
    "    for i in range(n):\n",
    "        if x[i] == max_array[max_indices[i]]:\n",
    "            result[i] = 1.0\n",
    "    \n",
    "    return max_array\n",
    "\n",
    "def _detect_peak_r(cnp.ndarray[cnp.float64_t, ndim=1] x, int k):\n",
    "    cdef int n = x.shape[0]\n",
    "    cdef cnp.ndarray[cnp.float64_t, ndim=1] max_array = np.zeros(n, dtype=np.float64)\n",
    "    cdef cnp.ndarray[cnp.int32_t, ndim=1] max_indices = np.zeros(n, dtype=np.int32)\n",
    "    cdef cnp.ndarray[cnp.float64_t, ndim=1] result = np.zeros(n, dtype=np.float64)\n",
    "    cdef int i, j, start, end, max_index\n",
    "    \n",
    "    # calculate max values in first window\n",
    "    max_index = 0\n",
    "    for i in range(k):\n",
    "        if x[i] > x[max_index]:\n",
    "            max_index = i\n",
    "    max_array[k-1] = x[max_index]\n",
    "    max_indices[k-1] = max_index\n",
    "    \n",
    "    # calculate max values in each window\n",
    "    for i in range(k, n):\n",
    "        start = i - k\n",
    "        end = i\n",
    "        if max_index == start - 1:\n",
    "            max_index = start\n",
    "            for j in range(start, end):\n",
    "                if x[j] > x[max_index]:\n",
    "                    max_index = j\n",
    "        else:\n",
    "            if x[i] > x[max_index]:\n",
    "                max_index = i\n",
    "        max_array[i] = x[max_index]\n",
    "        max_indices[i] = max_index\n",
    "    \n",
    "    # set peak values to 1\n",
    "    for i in range(n):\n",
    "        if x[i] == max_array[max_indices[i]]:\n",
    "            result[i] = 1.0\n",
    "    \n",
    "    return max_array\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "def detect_peak_kmat(cnp.ndarray[cnp.float64_t, ndim=1] x, int k):\n",
    "    cdef int n = x.shape[0]\n",
    "    cdef cnp.ndarray[cnp.float64_t, ndim=1] max_array = np.zeros(n, dtype=np.float64)\n",
    "    cdef cnp.ndarray[cnp.float64_t, ndim=1] result_val = np.zeros(n, dtype=np.float64)\n",
    "    cdef cnp.ndarray[cnp.float64_t, ndim=1] result = np.zeros(n, dtype=np.float64)\n",
    "    cdef int i, j, start, end, max_index, half_k\n",
    "\n",
    "    half_k = k // 2\n",
    "    for i in range(half_k, n-half_k):\n",
    "        result[i] = 1\n",
    "        result_val[i] = x[i]\n",
    "        for j in range(1, half_k+1):\n",
    "            if x[i] < x[i-j]:\n",
    "                result[i] = 0\n",
    "                result_val[i] = 0\n",
    "                break\n",
    "            if x[i] < x[i+j]:\n",
    "                result[i] = 0\n",
    "                result_val[i] = 0\n",
    "                break\n",
    "    return result, result_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### postprocess 1st stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def detect_peak(array, valid_mask, kernel_size=5, threshold = 0.05):\n",
    "    # old (GPU)\n",
    "    # array = tf.cast(array.reshape(1,-1,1), tf.float32)\n",
    "    # max_array = tf.nn.max_pool1d(array, ksize=kernel_size, strides=1, padding=\"SAME\")\n",
    "    # mask = tf.cast(max_array==array, tf.float32)\n",
    "    # array = (array*mask).numpy().reshape(-1)\n",
    "\n",
    "    peak_mask, peak_array = detect_peak_kmat(array.astype(np.float64), int(kernel_size))\n",
    "    pred_mask = peak_mask# * valid_mask\n",
    "    peak_array = peak_array# * valid_mask\n",
    "    peak_indices = np.where(peak_array>0)[0]\n",
    "    peak_val = peak_array[peak_indices]\n",
    "    mask_over_threshold = peak_val > threshold\n",
    "    peak_indices = peak_indices[mask_over_threshold]\n",
    "    peak_val = peak_val[mask_over_threshold]\n",
    "    peak_results = {\"peak_array\": peak_array, \"peak_indices\": peak_indices, \"peak_val\": peak_val}\n",
    "    # plt.figure(figsize=(15,4))\n",
    "    # plt.plot(array[20000:80000])\n",
    "    # plt.plot(peak_array[20000:80000])\n",
    "    # plt.show()\n",
    "    # raise Exception\n",
    "    return peak_results\n",
    "\n",
    "def gaussian_smooth(array, sigma):#\n",
    "    if sigma<=0:\n",
    "        return array\n",
    "    array = scipy.ndimage.gaussian_filter1d(array, sigma, mode='reflect')\n",
    "    return array\n",
    "\n",
    "def moving_average(array, kernel_size):\n",
    "    if kernel_size<=0:\n",
    "        return array\n",
    "    array = scipy.ndimage.uniform_filter1d(array, kernel_size, mode='reflect')\n",
    "    return array\n",
    "\n",
    "\n",
    "def smooth_awake(pred_awake, gauss_factor):\n",
    "    \"\"\"\n",
    "    gaussian smooth\n",
    "    \"\"\"\n",
    "    \n",
    "    return smooth_pred_awake\n",
    "\n",
    "def find_event(event_indices, pred_awake, before_length, after_length, awake_threshold=0.5, sleep_threshold=0.5, awake_sleep_diff_threshold=0, lengthlist_for_2ndstage=None, pred_nan=None, pred_scores=None, nans_feat=None):\n",
    "    \"\"\"\n",
    "    take average of state before and after event steps\n",
    "    \"\"\"\n",
    "    before_states = []\n",
    "    after_states = []\n",
    "    event_types = []\n",
    "    for event_index in event_indices:\n",
    "        before_state = pred_awake[max(0, event_index-before_length):event_index].mean()\n",
    "        after_state = pred_awake[event_index+1:min(event_index+after_length+1, len(pred_awake))].mean()\n",
    "        before_states.append(before_state)\n",
    "        after_states.append(after_state)\n",
    "        if before_state > after_state + awake_sleep_diff_threshold:\n",
    "            if before_state > awake_threshold and after_state < sleep_threshold:\n",
    "                event_type = \"onset\"\n",
    "            else:\n",
    "                event_type = \"nan\"\n",
    "        elif after_state > before_state + awake_sleep_diff_threshold:\n",
    "            if before_state < sleep_threshold and after_state > awake_threshold:\n",
    "                event_type = \"wakeup\"\n",
    "            else:\n",
    "                event_type = \"nan\"\n",
    "        else:\n",
    "            event_type = \"nan\"\n",
    "        event_types.append(event_type)\n",
    "    event_results = {\"event_indices\": event_indices, \"before_states\": before_states, \"after_states\": after_states, \"event_types\": event_types}\n",
    "\n",
    "\n",
    "    if lengthlist_for_2ndstage is not None:\n",
    "        for length in lengthlist_for_2ndstage:\n",
    "            event_results[f\"before_states_feat_{length}\"] = [pred_awake[max(0, event_index-length):event_index].mean() for event_index in event_indices]\n",
    "            event_results[f\"after_states_feat_{length}\"] = [pred_awake[event_index+1:min(event_index+length+1, len(pred_awake))].mean() for event_index in event_indices]\n",
    "            if pred_nan is not None:\n",
    "                event_results[f\"before_nan_feat_{length}\"] = [pred_nan[max(0, event_index-length):event_index].mean() for event_index in event_indices]\n",
    "                event_results[f\"after_nan_feat_{length}\"] = [pred_nan[event_index+1:min(event_index+length+1, len(pred_awake))].mean() for event_index in event_indices]\n",
    "            if pred_scores is not None:\n",
    "                for i, col in enumerate(['pred_switch', \"pred_switch10p\", \"pred_switch10\", \"pred_switch8\", \"pred_switch6\", \"pred_switch4\", \"pred_switch2\"]):\n",
    "                    # event_results[f\"before_{col}_{length//2}\"] = [pred_scores[max(0, event_index-length//2):event_index,i].mean() for event_index in event_indices]\n",
    "                    # event_results[f\"after_{col}_{length//2}\"] = [pred_scores[event_index+1:min(event_index+length//2+1, len(pred_awake)),i].mean() for event_index in event_indices]\n",
    "                    event_results[f\"befaf_{col}_{length//2}\"] = [pred_scores[max(0, event_index-length//2):event_index,i].mean()-pred_scores[event_index+1:min(event_index+length//2+1, len(pred_awake)),i].mean() for event_index in event_indices]\n",
    "                    # event_results[f\"{col}_{length//2}_std\"] = [pred_scores[max(0, event_index-length//2):min(event_index+length//2+1, len(pred_awake)),i].std() for event_index in event_indices]\n",
    "                    # 以下追加1120                    \n",
    "            # if nans_feat is not None:\n",
    "            #     for i, col in enumerate(['nan_counter', 'nan_span']):\n",
    "            #         event_results[f\"{col}_{length//2}_mean\"] = [nans_feat[max(0, event_index-length//2):min(event_index+length//2+1, len(pred_awake)),i].mean() for event_index in event_indices]\n",
    "            #         event_results[f\"{col}_{length//2}_min\"] = [nans_feat[max(0, event_index-length//2):min(event_index+length//2+1, len(pred_awake)),i].min() for event_index in event_indices]\n",
    "            #         event_results[f\"{col}_{length//2}_max\"] = [nans_feat[max(0, event_index-length//2):min(event_index+length//2+1, len(pred_awake)),i].max() for event_index in event_indices]\n",
    "\n",
    "    return event_results\n",
    "\n",
    "def add_night_group(out_df_single):\n",
    "    # daily 10000step == 2 pm -> night_no\n",
    "    out_df_single[\"offset_step\"] = out_df_single['step'] + out_df_single['daily_step'].iloc[0] * Cfg.step_for_a_day - out_df_single['step'].iloc[0]\n",
    "    out_df_single[\"night\"] = 1 + (out_df_single[\"offset_step\"] + Cfg.step_for_a_day - 10000) // Cfg.step_for_a_day\n",
    "    return out_df_single\n",
    "\n",
    "def add_night_features(out_df_single, columns=[\"pred_nan_counter\", \"pred_nan_span\", \"pred_nan\"]):\n",
    "    \"\"\"\n",
    "    groupby night features (for 2nd stage)\n",
    "    \"\"\"\n",
    "    out_df_single = add_night_group(out_df_single)\n",
    "    for col in columns:\n",
    "        out_df_single[f\"{col}_mean\"] = out_df_single.groupby(\"night\")[col].transform(\"mean\")\n",
    "        out_df_single[f\"{col}_min\"] = out_df_single.groupby(\"night\")[col].transform(\"min\")\n",
    "        out_df_single[f\"{col}_max\"] = out_df_single.groupby(\"night\")[col].transform(\"max\")\n",
    "    \n",
    "    out_df_single = out_df_single.drop(columns=[\"offset_step\", \"night\"])\n",
    "    return out_df_single\n",
    "\n",
    "\n",
    "def delete_inappropriate_event(out_df_single, prior_conf_dev=0.1, prior_conf_abs=0.5):\n",
    "    \"\"\"\n",
    "\n",
    "    many post process\n",
    "    -> finally, use second stage model instead of this function\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # nightとeventの組み合わせでgroupbyして、最大のscoreをカラムに追加する。差が大きい場合は削除する\n",
    "    out_df_single[\"max_score_in_night\"] = out_df_single.groupby([\"night\", \"event\"])[\"score\"].transform(\"max\")\n",
    "    out_df_single[\"score_diff\"] = out_df_single[\"max_score_in_night\"] - out_df_single[\"score\"]\n",
    "    mask_by_diff = out_df_single[\"score_diff\"] < prior_conf_dev\n",
    "    mask_by_max = (np.logical_and((out_df_single[\"score\"] != out_df_single[\"max_score_in_night\"]),  out_df_single[\"max_score_in_night\"] > prior_conf_abs))==False\n",
    "    mask = np.logical_and(mask_by_diff, mask_by_max)    \n",
    "    out_df_single[\"score\"] = out_df_single[\"score\"] * mask + (1-mask) * 0.1 * out_df_single[\"score\"]# 1/10で残す　テンポラリ\n",
    "    \n",
    "    # start-endを落とすところは元のやつを使いたい\n",
    "    # maskが最初にTrueになるindexを取得する\n",
    "    first_index = np.argmax(mask)\n",
    "    # maskが最後にTrueになるindexを取得する\n",
    "    last_index = len(mask) - np.argmax(mask[::-1])\n",
    "    out_df_single = out_df_single.iloc[first_index:last_index].copy()\n",
    "\n",
    "    # out_df_single = out_df_single[mask]\n",
    "\n",
    " \n",
    "    if len(out_df_single) == 0:\n",
    "        return out_df_single\n",
    "\n",
    "    # onsetの後のonset: wakeupが取れなかったケース。最初のonsetは削除\n",
    "    valid_mask = np.ones(len(out_df_single))\n",
    "    # print(len(out_df_single))\n",
    "\n",
    "    \n",
    "\n",
    "    current_best_conf = 0.\n",
    "    current_best_idx = 0\n",
    "    \n",
    "\n",
    "\n",
    "    onset_found = False\n",
    "    wakeup_found = False\n",
    "    count = 0\n",
    "    while not onset_found: # 最初のイベントがwake upだった場合は、最初のwake upを削除する\n",
    "        if out_df_single['event'].iloc[count] == 'onset':\n",
    "            onset_found = True\n",
    "        else:\n",
    "            valid_mask[count] = 0\n",
    "            count += 1\n",
    "            if count >= len(out_df_single):\n",
    "                break\n",
    "\n",
    "    count = 1\n",
    "    while not wakeup_found: # 最後のイベントがonsetだった場合は、最後のonsetを削除する\n",
    "        if out_df_single['event'].iloc[-count] == 'wakeup':\n",
    "            wakeup_found = True\n",
    "        else:\n",
    "            valid_mask[-count] = 0\n",
    "            count += 1\n",
    "            if count >= len(out_df_single):\n",
    "                break\n",
    "\n",
    "    \n",
    "    # out_df_single = out_df_single[valid_mask==1]\n",
    "    updated_score = out_df_single[\"score\"].values * valid_mask + (1-valid_mask) * 0.1 * out_df_single[\"score\"].values# 1/10で残す　テンポラリ\n",
    "    out_df_single.loc[:, \"score\"] = updated_score\n",
    "    # if len(out_df_single) == 0:\n",
    "    #     return out_df_single\n",
    "\n",
    "    # too long sleep\n",
    "    # out_df_single['sleep_length'] = np.repeat(out_df_single['step'].values[1::2] - out_df_single['step'].values[::2], 2)\n",
    "    # out_df_single = out_df_single[out_df_single['sleep_length'] < 14000]\n",
    "    # if len(out_df_single) == 0:\n",
    "    #     return out_df_single\n",
    "\n",
    "    # 同じnightでonsetもしくはwakeupのどちらかがない場合\n",
    "    out_df_single[\"num_onset\"] = out_df_single.groupby(\"night\")[\"event\"].transform(lambda x: (x==\"onset\").sum())\n",
    "    out_df_single[\"num_wakeup\"] = out_df_single.groupby(\"night\")[\"event\"].transform(lambda x: (x==\"wakeup\").sum())\n",
    "    out_df_single[\"have_sleep\"] = np.logical_and((out_df_single[\"num_onset\"]>0), out_df_single[\"num_wakeup\"]>0)\n",
    "    # out_df_single.loc[:, \"score\"] = out_df_single.loc[:, \"score\"] * out_df_single.loc[:, \"have_sleep\"] + (1-out_df_single.loc[:, \"have_sleep\"]) * 0.1 * out_df_single.loc[:, \"score\"]# 1/10で残す　テンポラリ\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    return out_df_single\n",
    "\n",
    "def avoid_6_vals(out_df_single, pred_switch, window_size=20):\n",
    "    \"\"\"\n",
    "    6の倍数のところは評価指標上僅かに損をする。(GTが1分単位であり、評価指標が12または6の倍数であるため)\n",
    "    予測結果のstepが6の倍数の場合は、そこからどちらかにずらすと得をする。\n",
    "    \"\"\"\n",
    "    # out_df_single['step'] = out_df_single['step'] + ((out_df_single['step']%6) == 0).astype(int)\n",
    "    out_df_single.reset_index(drop=True, inplace=True)\n",
    "    out_df_single[\"is_6_multiple\"] = (out_df_single['step']%6) == 0\n",
    "    shifts = np.zeros(len(out_df_single))\n",
    "    for i in range(len(out_df_single)-1):\n",
    "        if out_df_single['is_6_multiple'].iloc[i] == True:\n",
    "            index = out_df_single['peak_indices'].iloc[i]\n",
    "            before = pred_switch[max(0, index-window_size):index].mean()\n",
    "            after = pred_switch[index+1:min(index+window_size+1, len(pred_switch))].mean()\n",
    "            if before > after:\n",
    "                # out_df_single['step'].iloc[i] -= 1\n",
    "                # out_df_single.at[i, 'step']. -= 1\n",
    "                shifts[i] = -1\n",
    "            else:\n",
    "                # out_df_single['step'].iloc[i] += 1\n",
    "                # out_df_single.at[i, 'step']. += 1\n",
    "                shifts[i] = 1\n",
    "    out_df_single['step'] = out_df_single['step'] + shifts.astype(int)\n",
    "\n",
    "    # drop column\n",
    "    out_df_single = out_df_single.drop(columns=[\"is_6_multiple\"])\n",
    "    return out_df_single\n",
    "\n",
    "def avoid_6_vals_v2(out_df_single, pred_switch, window_size=20):\n",
    "    # 近い分にまるめてしまってから1stepずらす。決め打ちリスキー。\n",
    "\n",
    "    mins = (out_df_single['step'] % int(Cfg.step_for_15min)) / Cfg.step_for_1min\n",
    "    important_mins = [0,3,7,11,15]\n",
    "    minites_near = []\n",
    "    mins_diff = []\n",
    "    for m in important_mins:\n",
    "        mins_diff.append(mins - m)\n",
    "    mins_diff = np.array(mins_diff)\n",
    "    mins_diff_abs = np.abs(mins_diff)\n",
    "    mins_diff_argmin = np.argmin(mins_diff_abs, axis=0)\n",
    "    mins_near = np.array(important_mins)[mins_diff_argmin]\n",
    "    # print(mins_near)\n",
    "    min_mins_diff = mins_diff[mins_diff_argmin, np.arange(len(mins_diff_argmin))]\n",
    "    new_mins_step = (mins_near * Cfg.step_for_1min + (min_mins_diff > 0) - (min_mins_diff <= 0)).astype(int)\n",
    "    # print(new_mins_step.shape)\n",
    "    \n",
    "    out_df_single['step'] = (out_df_single['step'] // int(Cfg.step_for_15min)) * int(Cfg.step_for_15min) + new_mins_step\n",
    "    return out_df_single\n",
    "\n",
    "def predict_averaging(predictions, weights):\n",
    "    \"\"\"\n",
    "    予測値の平均をとる\n",
    "    peak予測をしたpred_10, pred_8, pred_6, pred_4, pred_2、それぞれの重みを与えて平均をとる\n",
    "    \"\"\"\n",
    "    # pred = np.zeros(len(predictions[0]))\n",
    "    # for prediction, weight in zip(predictions, weights):\n",
    "    #     pred += prediction * weight\n",
    "    # pred /= np.sum(weights)\n",
    "    pred = (predictions * np.array(weights).reshape(1,-1)).sum(axis=1)# / np.sum(weights)\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "def run_postprocess(df, peak_kernel_size=30, peak_threshold=0.05, awake_threshold=0.5, awake_sleep_diff_threshold=0, sleep_threshold=0.5, event_before_length=30, event_after_length=30, prior_conf_dev=0.1, averaging_weight=[0,1,0.5,0,0,0]):\n",
    "    \"\"\"\n",
    "    pp_params\n",
    "    - gauss_factor: gaussian smoothのfactor\n",
    "    - awake_threshold: 0.5\n",
    "    - sleep_threshold: 0.5\n",
    "    - peak_kernel_size: 30(min)\n",
    "    - peak_threshold: 0.05\n",
    "    - event_before_length: 30(min)\n",
    "    - event_after_length: 30(min)\n",
    "\n",
    "    \"\"\"\n",
    "    # dataframeは[\"step\", \"pred_awake\", \"pred_switch\", \"pred_nan\"})からなる\n",
    "    # 複数アウト\n",
    "    df[\"pred_switch\"] = predict_averaging(df[[\"pred_switch10p\", \"pred_switch10\", \"pred_switch8\", \"pred_switch6\", \"pred_switch4\", \"pred_switch2\"]].values, weights=averaging_weight)\n",
    "    df[\"pred_switch\"] = gaussian_smooth(df[\"pred_switch\"].values, sigma=4)\n",
    "    nonnan_mask = 1 - df[\"pred_nan\"].values\n",
    "    peak_results = detect_peak(df['pred_switch'].values, nonnan_mask, kernel_size=int(1+2*peak_kernel_size*Cfg.step_for_1min), threshold=peak_threshold) # 2*は両側\n",
    "    event_results = find_event(peak_results[\"peak_indices\"], df['pred_awake'].values, int(event_before_length*Cfg.step_for_1min), int(event_after_length*Cfg.step_for_1min), awake_threshold=awake_threshold, sleep_threshold=sleep_threshold, awake_sleep_diff_threshold=awake_sleep_diff_threshold)\n",
    "    out_df_single = pd.DataFrame({\"step\": df['step'].values[peak_results[\"peak_indices\"]], \"peak_indices\": peak_results[\"peak_indices\"], \"daily_step\": df['daily_step'].values[peak_results[\"peak_indices\"]], \"event\": event_results['event_types'], \"score\": peak_results['peak_val']})\n",
    "    # 一時的にevent==\"nan\"以外のみ残す\n",
    "    out_df_single = out_df_single[out_df_single['event']!=\"nan\"]\n",
    "    if len(out_df_single) > 0:\n",
    "        out_df_single = add_night_group(out_df_single)\n",
    "        out_df_single = delete_inappropriate_event(out_df_single, prior_conf_dev)\n",
    "    if len(out_df_single) > 0:\n",
    "        out_df_single = avoid_6_vals(out_df_single, df['pred_switch'].values)\n",
    "    #     # out_df_single = avoid_6_vals_v2(out_df_single, df['pred_switch'].values)\n",
    "        \n",
    "    return out_df_single\n",
    "\n",
    "def make_dataset_for_second_model(df_target, df, peak_kernel_size=30, peak_threshold=0.05, awake_threshold=0.5, awake_sleep_diff_threshold=0, sleep_threshold=0.5, event_before_length=30, event_after_length=30, prior_conf_dev=0.1, averaging_weight=[0,1,0.5,0,0,0]):\n",
    "\n",
    "    \"\"\"\n",
    "    マトリクスを作って予測に対して割り当てられるものを見つける\n",
    "    夜毎のグループで予測確信度でソートして、何個目までいくとスコアいくつで割り当てられるのか、もしくは全体を見た再スコアリングみたいなものをやってみたい。\n",
    "\n",
    "    予測点数が多すぎる場合は削るとか、他のやつのスコアも低い場合は押し上げるとか。\n",
    "    まずはポテンシャルをみたいところ\n",
    "    \"\"\"\n",
    "    lengthlist_for_2ndstage = [12, 24, 60, 120, 240, 360, 720] # sleep awakeの平均値保持用\n",
    "    night_agg_cols = [\"pred_nan_counter\", \"pred_nan_span\", \"pred_nan\", \"pred_switch2\", \"pred_switch\", \"pred_awake\"]# + [f\"sub_feat_{i}\" for i in range(11)]\n",
    "    df[\"pred_switch\"] = predict_averaging(df[[\"pred_switch10p\", \"pred_switch10\", \"pred_switch8\", \"pred_switch6\", \"pred_switch4\", \"pred_switch2\"]].values, weights=averaging_weight)\n",
    "    df[\"pred_switch\"] = gaussian_smooth(df[\"pred_switch\"].values, sigma=4)\n",
    "    df = add_night_features(df, columns=night_agg_cols)\n",
    "    nonnan_mask = 1 - df[\"pred_nan\"].values\n",
    "    peak_results = detect_peak(df['pred_switch'].values, nonnan_mask, kernel_size=int(1+2*peak_kernel_size*Cfg.step_for_1min), threshold=peak_threshold)\n",
    "    event_results = find_event(peak_results[\"peak_indices\"], df['pred_awake'].values, int(event_before_length*Cfg.step_for_1min), int(event_after_length*Cfg.step_for_1min), awake_threshold=awake_threshold, sleep_threshold=sleep_threshold, \n",
    "                                awake_sleep_diff_threshold=awake_sleep_diff_threshold, \n",
    "                                lengthlist_for_2ndstage=lengthlist_for_2ndstage,\n",
    "                                pred_nan=df['pred_nan'].values,\n",
    "                                pred_scores=df[['pred_switch', \"pred_switch10p\", \"pred_switch10\", \"pred_switch8\", \"pred_switch6\", \"pred_switch4\", \"pred_switch2\"]].values,\n",
    "                                nans_feat=df[[\"pred_nan_counter\", \"pred_nan_span\"]].values)\n",
    "    out_df_single = pd.DataFrame({\"step\": df['step'].values[peak_results[\"peak_indices\"]], \n",
    "            \"peak_indices\": peak_results[\"peak_indices\"], \n",
    "            \"daily_step\": df['daily_step'].values[peak_results[\"peak_indices\"]], \n",
    "            \"event\": event_results['event_types'], \n",
    "            \"score\": peak_results['peak_val'],\n",
    "            \"score_10p\": df['pred_switch10p'].values[peak_results[\"peak_indices\"]],\n",
    "            \"score_10\": df['pred_switch10'].values[peak_results[\"peak_indices\"]],\n",
    "            \"score_8\": df['pred_switch8'].values[peak_results[\"peak_indices\"]],\n",
    "            \"score_6\": df['pred_switch6'].values[peak_results[\"peak_indices\"]],\n",
    "            \"score_4\": df['pred_switch4'].values[peak_results[\"peak_indices\"]],\n",
    "            \"score_2\": df['pred_switch2'].values[peak_results[\"peak_indices\"]],\n",
    "            \"pred_nan\": df['pred_nan'].values[peak_results[\"peak_indices\"]],\n",
    "            \"pred_nan_counter\": df['pred_nan_counter'].values[peak_results[\"peak_indices\"]],\n",
    "            \"pred_nan_span\": df['pred_nan_span'].values[peak_results[\"peak_indices\"]],\n",
    "            \"pred_awake\": df['pred_awake'].values[peak_results[\"peak_indices\"]],\n",
    "            \"before_states\": event_results['before_states'],\n",
    "            \"after_states\": event_results['after_states'],\n",
    "            })\n",
    "    for length in lengthlist_for_2ndstage:\n",
    "        out_df_single[f\"before_states_feat_{length}\"] = event_results[f\"before_states_feat_{length}\"]\n",
    "        out_df_single[f\"after_states_feat_{length}\"] = event_results[f\"after_states_feat_{length}\"]\n",
    "        out_df_single[f\"before_nan_feat_{length}\"] = event_results[f\"before_nan_feat_{length}\"]\n",
    "        out_df_single[f\"after_nan_feat_{length}\"] = event_results[f\"after_nan_feat_{length}\"]\n",
    "        for i, col in enumerate(['pred_switch', \"pred_switch10p\", \"pred_switch10\", \"pred_switch8\", \"pred_switch6\", \"pred_switch4\", \"pred_switch2\"]):\n",
    "            # out_df_single[f\"before_{col}_{length//2}\"] = event_results[f\"before_{col}_{length//2}\"]\n",
    "            # out_df_single[f\"after_{col}_{length//2}\"] = event_results[f\"after_{col}_{length//2}\"]\n",
    "            out_df_single[f\"befaf_{col}_{length//2}\"] = event_results[f\"befaf_{col}_{length//2}\"]\n",
    "            # out_df_single[f\"{col}_{length//2}_std\"] = event_results[f\"{col}_{length//2}_std\"]\n",
    "            \n",
    "        # for i, col in enumerate(['nan_counter', 'nan_span']):\n",
    "        #     out_df_single[f\"{col}_{length//2}_mean\"] = event_results[f\"{col}_{length//2}_mean\"]\n",
    "        #     out_df_single[f\"{col}_{length//2}_min\"] = event_results[f\"{col}_{length//2}_min\"]\n",
    "        #     out_df_single[f\"{col}_{length//2}_max\"] = event_results[f\"{col}_{length//2}_max\"]\n",
    "    for col in night_agg_cols:\n",
    "        out_df_single[f\"{col}_mean\"] = df[f\"{col}_mean\"].values[peak_results[\"peak_indices\"]]\n",
    "        out_df_single[f\"{col}_min\"] = df[f\"{col}_min\"].values[peak_results[\"peak_indices\"]]\n",
    "        out_df_single[f\"{col}_max\"] = df[f\"{col}_max\"].values[peak_results[\"peak_indices\"]]\n",
    "    # 一時的にevent==\"nan\"以外のみ残す\n",
    "    out_df_single = out_df_single[out_df_single['event']!=\"nan\"]\n",
    "    if len(out_df_single) > 0:\n",
    "        out_df_single = add_night_group(out_df_single)\n",
    "\n",
    "    # shiftも先にのせてしまう。シフト値だけ記憶してもいいけど。\n",
    "    if len(out_df_single) > 0:\n",
    "        out_df_single = avoid_6_vals(out_df_single, df['pred_switch'].values)\n",
    "    \n",
    "    # add_target\n",
    "    def diff_step_to_AP(diff_step):\n",
    "        return np.sum(np.array([diff_step < threshold for threshold in [12, 36, 60, 90, 120, 150, 180, 240, 300, 360]]), axis=0)\n",
    "    if len(df_target) >0:\n",
    "        diff_step_matrix = df_target['step'].values.reshape(-1,1) - out_df_single['step'].values.reshape(1,-1)\n",
    "        diff_step_matrix = np.abs(diff_step_matrix)\n",
    "        min_diff_step = np.min(diff_step_matrix, axis=0)\n",
    "        out_df_single['min_diff_step'] = min_diff_step\n",
    "        out_df_single['best_ap'] = diff_step_to_AP(min_diff_step)/ 10.\n",
    "    else:\n",
    "        out_df_single['min_diff_step'] = np.nan\n",
    "        out_df_single['best_ap'] = 0\n",
    "\n",
    "    return out_df_single\n",
    "\n",
    "\n",
    "def make_dataset_for_second_model_ensemble(ensemble_files, train_events, pp_params, save_path=\"../data/df_second_model.feather\"):\n",
    "    cannot_finds = []\n",
    "    id_nos = []\n",
    "    scores = []\n",
    "    dates = []\n",
    "    num_events = []\n",
    "    diff_steps = []\n",
    "    num_ensemble = len(ensemble_files)\n",
    "    do_ensemble = True if num_ensemble > 1 else False\n",
    "    df_second_model = []\n",
    "    for i, pred_files_abc in enumerate(zip(*ensemble_files)):\n",
    "\n",
    "        id_no = os.path.basename(pred_files_abc[0]).split(\"_\")[1]\n",
    "        # sub_data = os.path.join(Cfg.preprocess_dir, \"id_\" + id_no+\"_subfeat.npy\")\n",
    "        df_pred_id = pd.read_parquet(pred_files_abc[0])\n",
    "        # sub_data = np.load(sub_data)\n",
    "        # add_cols = [f\"sub_feat_{i}\" for i in range(sub_data.shape[-1])]\n",
    "        # df_pred_id[add_cols] = sub_data\n",
    "        # print(sub_data.shape, df_pred_id.shape)\n",
    "\n",
    "        # ----- ENSEMBLE\n",
    "        if do_ensemble:\n",
    "            for pf in pred_files_abc[1:]:\n",
    "                df_pred_id_1 = pd.read_parquet(pf)\n",
    "                cols = [c for c in df_pred_id.columns if \"pred_switch\" in c] + [\"pred_awake\"]\n",
    "                for c in cols:\n",
    "                    df_pred_id[c] += df_pred_id_1[c]\n",
    "            for c in cols:\n",
    "                df_pred_id[c] /= num_ensemble\n",
    "        # -----\n",
    "\n",
    "        print(id_no)\n",
    "        \n",
    "        solution =  train_events.loc[train_events['series_id']==id_no].copy()\n",
    "        solution = solution[~np.isnan(solution['step'])]\n",
    "\n",
    "        out_df_single = make_dataset_for_second_model(solution, df_pred_id, **pp_params)\n",
    "        out_df_single['series_id'] = id_no\n",
    "        df_second_model.append(out_df_single)\n",
    "        \n",
    "    df_second_model = pd.concat(df_second_model).reset_index(drop=True)\n",
    "    float32_cols = [col for col in df_second_model.columns if df_second_model[col].dtype==\"float64\"]\n",
    "    df_second_model[float32_cols] = df_second_model[float32_cols].astype(np.float32)\n",
    "    # feather で保存\n",
    "    df_second_model.to_feather(save_path)\n",
    "    return df_second_model\n",
    "\n",
    "def validation_scoring(pred_files, train_events, pp_params, return_df=False):\n",
    "    out_df = []\n",
    "    for file in pred_files:\n",
    "        id_no = os.path.basename(file).split(\"_\")[1]\n",
    "        df_pred_id = pd.read_parquet(file)\n",
    "        # df_pred_id[\"pred_switch\"] = df_pred_id[\"pred_switch\"]\n",
    "\n",
    "        # tmp = train_events[train_events['series_id']==id_no].copy()\n",
    "        # tmp = tmp[~np.isnan(tmp['step'])]\n",
    "        # max_step = tmp[\"step\"].max()\n",
    "        # df_pred_id = df_pred_id[df_pred_id['step'] <= max_step+2000] 終了タイミングわかれば2ポイントあがるのに\n",
    "\n",
    "        \n",
    "        out_df_single = run_postprocess(df_pred_id, **pp_params)\n",
    "        out_df_single['series_id'] = id_no\n",
    "        out_df.append(out_df_single)\n",
    "\n",
    "    out_df = pd.concat(out_df).reset_index(drop=True).reset_index().rename(columns={\"index\": \"row_id\"})\n",
    "    \n",
    "    series_ids = out_df['series_id'].unique()\n",
    "    print(\"num series\", len(series_ids))\n",
    "    solution =  train_events.loc[train_events['series_id'].isin(series_ids)].copy()\n",
    "    solution = solution[~np.isnan(solution['step'])]\n",
    "    total_score = event_detection_ap(solution, out_df.copy(), tolerances)\n",
    "    if return_df:\n",
    "        return total_score, out_df\n",
    "    else:\n",
    "        return total_score\n",
    "\n",
    "def weighted_fusion_ennsemble(df_0, df_1, distance_threshold=12):\n",
    "    series_ids = df_0['series_id'].unique()\n",
    "    out_df = []\n",
    "    for series_id in series_ids:\n",
    "        df_0_id = df_0[df_0['series_id']==series_id].copy()\n",
    "        df_1_id = df_1[df_1['series_id']==series_id].copy()\n",
    "        df_0_id = df_0_id.sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
    "        df_1_id = df_1_id.sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        steps_0 = df_0_id['step'].values # base\n",
    "        steps_1 = df_1_id['step'].values\n",
    "        scores_0 = df_0_id['score'].values # base\n",
    "        scores_1 = df_1_id['score'].values\n",
    "        for step, score in zip(steps_1, scores_1):\n",
    "            dists = np.abs(steps_0 - step)\n",
    "            argmin = np.argmin(dists)\n",
    "            min_dist = dists[argmin]\n",
    "            if min_dist < distance_threshold:\n",
    "                f_step = steps_0[argmin]\n",
    "                f_score = scores_0[argmin]\n",
    "                add_step = step\n",
    "                add_score = score\n",
    "                \n",
    "                new_score = (f_score + add_score)# / 2\n",
    "                new_step = ((f_step * f_score) + (add_step * add_score)) / (f_score + add_score)\n",
    "                df_0_id.loc[argmin, \"score\"] = new_score\n",
    "                df_0_id.loc[argmin, \"step\"] = new_step\n",
    "            else:\n",
    "                df_0_id = df_0_id.append(df_1_id[df_1_id['step']==step])\n",
    "            # assignされたもとのものは参照されないようにする\n",
    "            steps_0[argmin] = 1e10\n",
    "        out_df.append(df_0_id)\n",
    "    out_df = pd.concat(out_df).reset_index(drop=True).reset_index() # .rename(columns={\"index\": \"row_id\"})\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def validation_scoring_ensemble(pred_files, train_events, pp_params, return_df=False):\n",
    "    num_ensemble = len(pred_files)\n",
    "    out_df = []\n",
    "    print(\"no sort ensemble\")\n",
    "    # pred_files = [sorted(f) for f in pred_files]\n",
    "    num_files = len(pred_files[0])\n",
    "    for i in range(num_files):\n",
    "        id_no = os.path.basename(pred_files[0][i]).split(\"_\")[1]\n",
    "        \n",
    "        for j in range(num_ensemble):\n",
    "            if j==0:\n",
    "                df_pred_id = pd.read_parquet(pred_files[j][i])\n",
    "            else:\n",
    "                _df_pred_id = pd.read_parquet(pred_files[j][i])\n",
    "                cols = [c for c in _df_pred_id.columns if \"pred_switch\" in c]\n",
    "                for c in cols:\n",
    "                    df_pred_id[c] += _df_pred_id[c]\n",
    "        \n",
    "        for c in cols:\n",
    "            df_pred_id[c] /= num_ensemble\n",
    "        out_df_single = run_postprocess(df_pred_id, **pp_params)\n",
    "        out_df_single['series_id'] = id_no\n",
    "        out_df.append(out_df_single)\n",
    "    \n",
    "    out_df = pd.concat(out_df).reset_index(drop=True).reset_index().rename(columns={\"index\": \"row_id\"})\n",
    "\n",
    "    series_ids = out_df['series_id'].unique()\n",
    "    solution =  train_events.loc[train_events['series_id'].isin(series_ids)].copy()\n",
    "    solution = solution[~np.isnan(solution['step'])]\n",
    "    total_score = event_detection_ap(solution, out_df.copy(), tolerances)\n",
    "    if return_df:\n",
    "        return total_score, out_df\n",
    "    else:\n",
    "        return total_score\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# tolerances in steps\n",
    "tolerances = {\n",
    "    \"onset\":  [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],\n",
    "    \"wakeup\": [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],\n",
    "}\n",
    "# 12は(12の倍数)からだと1minしか許容されない。少しずらすだけで2min可能になる\n",
    "# 90と150は(12の倍数+6)からだとよくないか。ということでピッタリと30secは避ける。(6の倍数秒は避ける)\n",
    "\n",
    "series_id_column_name = \"series_id\"\n",
    "time_column_name = \"step\"\n",
    "event_column_name = \"event\"\n",
    "score_column_name = \"score\"\n",
    "use_scoring_intervals = None\n",
    "\n",
    "def score(\n",
    "        solution: pd.DataFrame,\n",
    "        submission: pd.DataFrame,\n",
    "        tolerances: Dict[str, List[float]],\n",
    "        series_id_column_name: str,\n",
    "        time_column_name: str,\n",
    "        event_column_name: str,\n",
    "        score_column_name: str,\n",
    "        use_scoring_intervals: bool = False,\n",
    ") -> float:\n",
    "    \n",
    "    # Validate metric parameters\n",
    "    assert len(tolerances) > 0, \"Events must have defined tolerances.\"\n",
    "    assert set(tolerances.keys()) == set(solution[event_column_name]).difference({'start', 'end'}),\\\n",
    "        (f\"Solution column {event_column_name} must contain the same events \"\n",
    "         \"as defined in tolerances.\")\n",
    "    assert pd.api.types.is_numeric_dtype(solution[time_column_name]),\\\n",
    "        f\"Solution column {time_column_name} must be of numeric type.\"\n",
    "\n",
    "    # Validate submission format\n",
    "    for column_name in [\n",
    "        series_id_column_name,\n",
    "        time_column_name,\n",
    "        event_column_name,\n",
    "        score_column_name,\n",
    "    ]:\n",
    "        if column_name not in submission.columns:\n",
    "            raise ParticipantVisibleError(f\"Submission must have column '{target_name}'.\")\n",
    "\n",
    "    if not pd.api.types.is_numeric_dtype(submission[time_column_name]):\n",
    "        raise ParticipantVisibleError(\n",
    "            f\"Submission column '{time_column_name}' must be of numeric type.\"\n",
    "        )\n",
    "    if not pd.api.types.is_numeric_dtype(submission[score_column_name]):\n",
    "        raise ParticipantVisibleError(\n",
    "            f\"Submission column '{score_column_name}' must be of numeric type.\"\n",
    "        )\n",
    "\n",
    "    # Set these globally to avoid passing around a bunch of arguments\n",
    "    globals()['series_id_column_name'] = series_id_column_name\n",
    "    globals()['time_column_name'] = time_column_name\n",
    "    globals()['event_column_name'] = event_column_name\n",
    "    globals()['score_column_name'] = score_column_name\n",
    "    globals()['use_scoring_intervals'] = use_scoring_intervals\n",
    "\n",
    "    return event_detection_ap(solution, submission, tolerances)\n",
    "\n",
    "def event_detection_ap(\n",
    "        solution: pd.DataFrame,\n",
    "        submission: pd.DataFrame,\n",
    "        tolerances: Dict[str, List[float]],\n",
    ") -> float:\n",
    "    # Ensure solution and submission are sorted properly\n",
    "    solution = solution.sort_values([series_id_column_name, time_column_name])\n",
    "    submission = submission.sort_values([series_id_column_name, time_column_name])\n",
    "\n",
    "    # Extract scoring intervals.\n",
    "    if use_scoring_intervals:\n",
    "        intervals = (\n",
    "            solution\n",
    "            .query(\"event in ['start', 'end']\")\n",
    "            .assign(interval=lambda x: x.groupby([series_id_column_name, event_column_name]).cumcount())\n",
    "            .pivot(\n",
    "                index='interval',\n",
    "                columns=[series_id_column_name, event_column_name],\n",
    "                values=time_column_name,\n",
    "            )\n",
    "            .stack(series_id_column_name)\n",
    "            .swaplevel()\n",
    "            .sort_index()\n",
    "            .loc[:, ['start', 'end']]\n",
    "            .apply(lambda x: pd.Interval(*x, closed='both'), axis=1)\n",
    "        )\n",
    "\n",
    "    # Extract ground-truth events.\n",
    "    ground_truths = (\n",
    "        solution\n",
    "        .query(\"event not in ['start', 'end']\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Map each event class to its prevalence (needed for recall calculation)\n",
    "    class_counts = ground_truths.value_counts(event_column_name).to_dict()\n",
    "\n",
    "    # Create table for detections with a column indicating a match to a ground-truth event\n",
    "    detections = submission.assign(matched = False)\n",
    "\n",
    "    # Remove detections outside of scoring intervals\n",
    "    if use_scoring_intervals:\n",
    "        detections_filtered = []\n",
    "        for (det_group, dets), (int_group, ints) in zip(\n",
    "            detections.groupby(series_id_column_name), intervals.groupby(series_id_column_name)\n",
    "        ):\n",
    "            assert det_group == int_group\n",
    "            detections_filtered.append(filter_detections(dets, ints))\n",
    "        detections_filtered = pd.concat(detections_filtered, ignore_index=True)\n",
    "    else:\n",
    "        detections_filtered = detections\n",
    "    # Create table of event-class x tolerance x series_id values\n",
    "    aggregation_keys = pd.DataFrame(\n",
    "        [(ev, tol, vid)\n",
    "         for ev in tolerances.keys()\n",
    "         for tol in tolerances[ev]\n",
    "         for vid in ground_truths[series_id_column_name].unique()],\n",
    "        columns=[event_column_name, 'tolerance', series_id_column_name],\n",
    "    )\n",
    "\n",
    "    # Create match evaluation groups: event-class x tolerance x series_id\n",
    "    detections_grouped = (\n",
    "        aggregation_keys\n",
    "        .merge(detections_filtered, on=[event_column_name, series_id_column_name], how='left')\n",
    "        .groupby([event_column_name, 'tolerance', series_id_column_name])\n",
    "    )\n",
    "    ground_truths_grouped = (\n",
    "        aggregation_keys\n",
    "        .merge(ground_truths, on=[event_column_name, series_id_column_name], how='left')\n",
    "        .groupby([event_column_name, 'tolerance', series_id_column_name])\n",
    "    )\n",
    "    # Match detections to ground truth events by evaluation group\n",
    "    detections_matched = []\n",
    "    for key in aggregation_keys.itertuples(index=False):\n",
    "        dets = detections_grouped.get_group(key)\n",
    "        gts = ground_truths_grouped.get_group(key)\n",
    "        detections_matched.append(\n",
    "            match_detections(dets['tolerance'].iloc[0], gts, dets)\n",
    "        )\n",
    "    detections_matched = pd.concat(detections_matched)\n",
    "    \n",
    "\n",
    "    # Compute AP per event x tolerance group\n",
    "    event_classes = ground_truths[event_column_name].unique()\n",
    "    ap_table = (\n",
    "        detections_matched\n",
    "        .query(\"event in @event_classes\")\n",
    "        .groupby([event_column_name, 'tolerance']).apply(\n",
    "            lambda group: average_precision_score(\n",
    "                group['matched'].to_numpy(),\n",
    "                group[score_column_name].to_numpy(),\n",
    "                class_counts[group[event_column_name].iat[0]],\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    # Average over tolerances, then over event classes\n",
    "    mean_ap = ap_table.groupby(event_column_name).mean().sum() / len(event_classes)\n",
    "\n",
    "    return mean_ap\n",
    "\n",
    "def _match_detections(\n",
    "        tolerance: float, ground_truths: pd.DataFrame, detections: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Match detections to ground truth events. Arguments are taken from a common event x tolerance x series_id evaluation group.\"\"\"\n",
    "    detections_sorted = detections.sort_values(score_column_name, ascending=False).dropna()\n",
    "    is_matched = np.full_like(detections_sorted[event_column_name], False, dtype=bool)\n",
    "    gts_matched = set()\n",
    "    for i, det in enumerate(detections_sorted.itertuples(index=False)):\n",
    "        best_error = tolerance\n",
    "        best_gt = None\n",
    "\n",
    "        for gt in ground_truths.itertuples(index=False):\n",
    "            error = abs(getattr(det, time_column_name) - getattr(gt, time_column_name))\n",
    "            if error < best_error and gt not in gts_matched:\n",
    "                best_gt = gt\n",
    "                best_error = error\n",
    "\n",
    "        if best_gt is not None:\n",
    "            is_matched[i] = True\n",
    "            gts_matched.add(best_gt)\n",
    "\n",
    "    detections_sorted['matched'] = is_matched\n",
    "\n",
    "    return detections_sorted\n",
    "\n",
    "def match_detections(\n",
    "        tolerance: float, ground_truths: pd.DataFrame, detections: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Match detections to ground truth events. Arguments are taken from a common event x tolerance x series_id evaluation group.\"\"\"\n",
    "    detections_sorted = detections.sort_values(score_column_name, ascending=False).dropna()\n",
    "    is_matched = np.full_like(detections_sorted[event_column_name], False, dtype=bool)\n",
    "    gts_matched = set()\n",
    "\n",
    "    det_times = detections_sorted[time_column_name].values.reshape(-1)\n",
    "    gt_times = ground_truths[time_column_name].values.reshape(-1)\n",
    "    errors_matrix = np.abs(gt_times[np.newaxis, :] - det_times[:, np.newaxis])\n",
    "    \"\"\"\n",
    "    pred_indices_matrix = np.tile(np.arange(len(det_times))[:,np.newaxis], (1,len(gt_times)))\n",
    "    gt_values_matrix = np.tile(gt_times[np.newaxis, :], (len(det_times),1))\n",
    "\n",
    "    cond = errors_matrix < tolerance\n",
    "    errors = errors_matrix[cond]\n",
    "    pred_indices = pred_indices_matrix[cond]\n",
    "    gt_values = gt_values_matrix[cond]\n",
    "    last_i = -1\n",
    "    best_gt = None\n",
    "    best_error = tolerance\n",
    "    for i, gt, error in zip(pred_indices, gt_values, errors):\n",
    "        pred_reset = True if i != last_i else False\n",
    "        if pred_reset and best_gt is not None:\n",
    "            is_matched[i] = True\n",
    "            gts_matched.add(best_gt)\n",
    "        if pred_reset:\n",
    "            best_gt = None\n",
    "            best_error = tolerance\n",
    "        if error < best_error and gt not in gts_matched:\n",
    "            best_gt = gt\n",
    "            best_error = error\n",
    "\n",
    "        last_i = i\n",
    "    if pred_reset and best_gt is not None:\n",
    "        is_matched[i] = True\n",
    "        gts_matched.add(best_gt)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # for i, det_time in enumerate(det_times): # detections_sorted.itertuples(index=False)):\n",
    "    for i in range(len(det_times)): # detections_sorted.itertuples(index=False)):\n",
    "\n",
    "        errors = errors_matrix[i] # np.abs(gt_times - det_time)\n",
    "        # print(errors.shape)\n",
    "        # print(np.abs(gt_times - det_time).shape)\n",
    "        # mask = (errors < tolerance) & (~np.isin(gt_indices, list(gts_matched)))\n",
    "        \n",
    "\n",
    "        best_error = tolerance\n",
    "        best_gt = None\n",
    "        mask = errors < best_error\n",
    "        errors_masked = errors[mask]\n",
    "        gt_times_masked = gt_times[mask]\n",
    "\n",
    "        for error, gt in zip(errors_masked, gt_times_masked):#ground_truths.itertuples(index=False):\n",
    "            # error = abs(getattr(det, time_column_name) - getattr(gt, time_column_name))\n",
    "            if  gt not in gts_matched:\n",
    "                best_gt = gt\n",
    "                best_error = error\n",
    "\n",
    "        if best_gt is not None:\n",
    "            is_matched[i] = True\n",
    "            gts_matched.add(best_gt)\n",
    "    \n",
    "\n",
    "    detections_sorted['matched'] = is_matched\n",
    "\n",
    "    return detections_sorted\n",
    "\n",
    "\n",
    "def precision_recall_curve(\n",
    "        matches: np.ndarray, scores: np.ndarray, p: int\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    if len(matches) == 0:\n",
    "        return [1], [0], []\n",
    "\n",
    "    # Sort matches by decreasing confidence\n",
    "    idxs = np.argsort(scores, kind='stable')[::-1]\n",
    "    scores = scores[idxs]\n",
    "    matches = matches[idxs]\n",
    "\n",
    "    distinct_value_indices = np.where(np.diff(scores))[0]\n",
    "    threshold_idxs = np.r_[distinct_value_indices, matches.size - 1]\n",
    "    thresholds = scores[threshold_idxs]\n",
    "\n",
    "    # Matches become TPs and non-matches FPs as confidence threshold decreases\n",
    "    tps = np.cumsum(matches)[threshold_idxs]\n",
    "    fps = np.cumsum(~matches)[threshold_idxs]\n",
    "\n",
    "    precision = tps / (tps + fps)\n",
    "    precision[np.isnan(precision)] = 0\n",
    "    recall = tps / p  # total number of ground truths might be different than total number of matches\n",
    "\n",
    "    # Stop when full recall attained and reverse the outputs so recall is non-increasing.\n",
    "    last_ind = tps.searchsorted(tps[-1])\n",
    "    sl = slice(last_ind, None, -1)\n",
    "\n",
    "    # Final precision is 1 and final recall is 0\n",
    "    return np.r_[precision[sl], 1], np.r_[recall[sl], 0], thresholds[sl]\n",
    "\n",
    "def average_precision_score(matches: np.ndarray, scores: np.ndarray, p: int) -> float:\n",
    "    precision, recall, _ = precision_recall_curve(matches, scores, p)\n",
    "    # Compute step integral\n",
    "    return -np.sum(np.diff(recall) * np.array(precision)[:-1])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_events = pd.read_csv(Cfg.train_target_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n",
      "num series 138\n",
      "0.8108576871662161 (13373, 14)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "pp_params = {'awake_threshold': 0.01118579240197301, 'sleep_threshold': 0.9741809981418406, 'awake_sleep_diff_threshold': 0.0036734165080717115, 'peak_kernel_size': 4.689964359639259, 'peak_threshold': 0.05135826117540523, 'event_before_length': 68.38372582053368, 'event_after_length': 99.69200455879107, 'prior_conf_dev': 0.29102092096947463}\n",
    "pp_params[\"averaging_weight\"] = [0.333, 0.333, 0.333, 0.00, 0, 0]\n",
    "\n",
    "pred_files =  glob.glob(\"../model/weights/exp04_run_14_SplitStem2fold35epStateSigmoidStratified14000stepDeepSEED111ManyposCh40Lr12_fold0/pred090/*.parquet\")# + glob.glob(\"../model/weights/exp04_run_14_SplitStem2fold35epStateSigmoidStratified14000stepDeepSEED111ManyposCh40Lr12_fold1/pred090/*.parquet\") # 0.8142088586829181 (31237, 14)\n",
    "pred_files =  glob.glob(\"../model/weights/exp04_run_20_SplitStem2fold35epStateSigmoidStratified18000stepDeepSEED42_fold0/pred090/*.parquet\")# + glob.glob(\"../model/weights/exp04_run_20_SplitStem2fold35epStateSigmoidStratified18000stepDeepSEED42_fold1/pred090/*.parquet\")\n",
    "\n",
    "pred_files =  glob.glob(\"../model/weights/exp04_run_24_SplitStem2foldSEED111controledStride_fold0/pred090/*.parquet\") + glob.glob(\"../model/weights/exp04_run_24_SplitStem2foldSEED111controledStride_fold1/pred090/*.parquet\") # 0.8145448837706926 (31883, 14) 0.814454051763341 (29676, 14)peak 尖らし。\n",
    "pred_files =  glob.glob(\"../model/weights/exp04_run_26_SplitStem2foldSEED111controledStride60secdata_fold0/pred090/*.parquet\") + glob.glob(\"../model/weights/exp04_run_26_SplitStem2foldSEED111controledStride60secdata_fold1/pred090/*.parquet\") # 0.8131842985751643 (29461, 14)\n",
    "pred_files =  glob.glob(\"../model/weights/exp04_run_28_SplitStem2foldSEED42controledStride_fold0/pred090/*.parquet\") + glob.glob(\"../model/weights/exp04_run_28_SplitStem2foldSEED42controledStride_fold1/pred090/*.parquet\") # \n",
    "\n",
    "\n",
    "# pred_files =  glob.glob(\"../model/weights/exp03_run_08_s10s8s6s4s2multiouts2fold40epTriModel_fold0/pred/*.parquet\")\n",
    "\n",
    "# pp_params = {'awake_threshold': 0.01118579240197301, 'sleep_threshold': 0.9741809981418406, 'awake_sleep_diff_threshold': 0.0036734165080717115, 'peak_kernel_size': 2, 'peak_threshold': 0.05135826117540523/2, 'event_before_length': 68.38372582053368/2, 'event_after_length': 99.69200455879107/2, 'prior_conf_dev': 0.29102092096947463}\n",
    "# pred_files = glob.glob(\"../model/weights/exp02_run_29_s10s8s6s4s2multiouts_25D40epPEncSin4STRIDE2_fold0/pred/*.parquet\")# + glob.glob(\"../model/weights/exp02_run_26_s10s8s6s4s2multiouts_25D40epPEncSin4_fold1/pred/*.parquet\") # 0.8055317525988136\n",
    "\n",
    "\n",
    "# pred_files = []\n",
    "# for i in range(5):\n",
    "#    pred_files += glob.glob(f\"../model/weights/exp02_run_09_focal_s10s8blend_cut3mix_5FOLD_fold{i}/pred/*.parquet\") # 0.7893816681055978 0.7895854315906429 0.7900825314890543, 0.7914190667754986\n",
    "\n",
    "# pred_files = []\n",
    "# for i in range(5):\n",
    "#    pred_files += glob.glob(f\"../model/weights/exp02_run_30_s10s8s6s4s2multiouts5fold_fold{i}/pred/*.parquet\") # 0.8101882890461795 gauss 0.8113963182104299\n",
    "\n",
    "# pred_files = []\n",
    "# for i in range(5):\n",
    "#    pred_files += glob.glob(f\"../model/weights/exp03_run_12_s10s8s6s4s2multiouts2fold35epNormal_fold{i}/pred/*.parquet\") # 0.8134289288683368 29719 (0.8144359591850177 , 43014) -> 0.8220663524837813 @ second stage\n",
    "\n",
    "# pred_files = []\n",
    "# for i in range(5):\n",
    "#    pred_files += glob.glob(f\"../model/weights/exp04_run_08_SplitStem2fold35epStateFocal25pStratified_fold{i}/pred/*.parquet\") # 0.813979615059301 31202 (0.8147061899749469, 46319)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# pred_files = []\n",
    "# for i in range(5):\n",
    "#    pred_files += glob.glob(f\"../model/weights/exp04_run_10_SplitStem2fold35epStateFocal25pStratified14000stepDeepSEED111_fold{i}/pred090/*.parquet\") # 0.8190526163294717 \n",
    "# print(len(pred_files))\n",
    "# pred_A = pred_files\n",
    "\n",
    "# pred_files = []\n",
    "# for i in range(5):\n",
    "#    pred_files += glob.glob(f\"../model/weights/exp04_run_11_SplitStem2fold35epStateFocal25pStratified14000stepDeepSEED42_fold{i}/pred/*.parquet\") # 0.8167204674976697 (31879, 14)\n",
    "\n",
    "# pred_B = pred_files\n",
    "\n",
    "# pred_files = []\n",
    "# for i in range(5):\n",
    "#    pred_files += glob.glob(f\"../model/weights/exp04_run_15_SplitStem2fold35epStateSigmoidStratified14000stepDeepSEED111ManyposCh40Lr12_fold{i}/pred090/*.parquet\") # 0.8210976525766087 (20031, 14) -> 0.8185172733765049 (19338, 14)\n",
    "\n",
    "pred_files = []\n",
    "for i in range(1,2):\n",
    "    # pred_files += glob.glob(f\"../model/weights/exp04_run_16_SplitStem2fold35epStateSigmoidStratified14000stepDeepSEED42ManyposCh40Lr12_fold{i}/pred090/*.parquet\") # 0.8186020290947744 (30023, 14)\n",
    "    # pred_files += glob.glob(f\"../model/weights/exp04_run_15_SplitStem2fold35epStateSigmoidStratified14000stepDeepSEED111ManyposCh40Lr12_fold{i}/pred090/*.parquet\") # 0.8202121238443345 (30185, 14)\n",
    "    # pred_files += glob.glob(f\"../model/weights/exp04_run_29_SplitStem5foldSEED111controledStride_fold{i}/pred090/*.parquet\")# seed42 0.8202335795874665 (30831, 14), seed 111 0.8202097089235567 (30409, 14)\n",
    "    # pred_files += glob.glob(f\"../model/weights/exp04_run_30_SplitStem2foldSEED111shift_target_fold{i}/pred090/*.parquet\")# seed42 0.8169063870836835 (30998, 14), seed111 0.8149626248679888 (30780, 14)\n",
    "\n",
    "    # pred_files += glob.glob(f\"../model/weights/exp04_run_36_SplitStem5foldSEED0controledStridev2_fold{i}/pred090/*.parquet\") # 42: 0.8212767417753822 (30821, 14) 111:0.8231633388171156 (30972, 14) 0:0.8226272950876579 (31140, 14)\n",
    "    pred_files += glob.glob(f\"../model/weights/exp04_run_38_SplitStem2foldSEED42normal_fold{i}/pred090/*.parquet\") # \n",
    "\n",
    "\n",
    "# pp_params[\"averaging_weight\"] = [0.333+0.1, 0.333-0.04, 0.333-0.04, 0.00, 0, 0] # 0.8211263988238442 (29130, 14)\n",
    "\n",
    "# pp_params[\"averaging_weight\"] = [0.33, 0.33, 0.33, 0.00, 0, 0] # 0.8213470468009865 (30300, 14) 0.823193701744753 (30422, 14)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# pp_params[\"averaging_weight\"] = [0.1, 0.3, 0.4, 0.2, 0.0, 0] # 0.8170802292733318 (31964, 14) 0.8168524707260241 (30507, 14) 0.8169063870836835 (30998, 14)\n",
    "# pp_params['peak_threshold'] += 0.012\n",
    "\n",
    "# pp_params[\"averaging_weight\"] = [0.333, 0.333, 0.333, 0.00, 0, 0] # 0.8153428343652527 (30980, 14)\n",
    "\n",
    "# pp_params[\"averaging_weight\"] = [0.133, 0.333, 0.333, 0.20, 0, 0] # 0.8165805440854592 (37663, 14) + 0.005 0.8163801737141492 (33827, 14) 0.8163981574272584 (31083, 14)\n",
    "\n",
    "\n",
    "print(len(pred_files))\n",
    "\n",
    "\n",
    "# pred_A_base = [os.path.basename(f) for f in pred_A]\n",
    "# pred_B_base = [os.path.basename(f) for f in pred_B]\n",
    "# pred_A = [pred_A[i] for i in np.argsort(pred_A_base)]\n",
    "# pred_B = [pred_B[i] for i in np.argsort(pred_B_base)]\n",
    "\n",
    "\n",
    "# pp_params = {'awake_threshold': 0.6504054224129343, 'sleep_threshold': 0.9567267743274901, 'peak_kernel_size': 48.42466176636616, 'peak_threshold': 0.08966659103875652, 'event_before_length': 38.308094765905594, 'event_after_length': 57.22569409093988, 'prior_conf_dev': 0.19796846319117362}\n",
    "\n",
    "debugging = False\n",
    "if debugging:\n",
    "    ref = ['038441c925bb', '03d92c9f6f8a', '0402a003dae9', '04f547b8017d',\n",
    "        '05e1944c3818', '062cae666e2a', '062dbd4c95e6', '08db4255286f',\n",
    "        '0a96f4993bd7', '0cd1e3d0ed95', '0ce74d6d2106', '0cfc06c129cc',\n",
    "        '0d0ad1e77851', '0dee4fda51c3', '0ec9fc461819', '0ef7d94fde99',\n",
    "        '0f572d690310', '0f9e60a8e56d', '10469f6765bf', '1087d7b0ff2e']\n",
    "    # ref = ['038441c925bb', '03d92c9f6f8a', '0402a003dae9', '04f547b8017d',\n",
    "    #    '05e1944c3818', '062cae666e2a', '062dbd4c95e6', '08db4255286f',\n",
    "    #    '0a96f4993bd7', '0cd1e3d0ed95', '0ce74d6d2106', '0cfc06c129cc',\n",
    "    #    '0d0ad1e77851', '0dee4fda51c3', '0ec9fc461819', '0ef7d94fde99',\n",
    "    #    '0f572d690310', '0f9e60a8e56d', '10469f6765bf', '1087d7b0ff2e',\n",
    "    #    '10f8bc1f7b07', '12d01911d509', '1319a1935f48', '137771d19ca2',\n",
    "    #    '137b99e936ab', '13b4d6a01d27', '148471991ffb', '154fe824ed87',\n",
    "    #    '16fe2798ed0f', '1716cd4163b2', '1762ab70ec76', '188d4b7cd28b',\n",
    "    #    '18a0ca03431d', '18b61dd5aae8', '1955d568d987', '1b92be89db4c',\n",
    "    #    '1c7c0bad1263', '1d4569cbac0f', '1e6717d93c1d', '1f96b9668bdf',\n",
    "    #    '207eded97727', '25e2b3dd9c3b', '2654a87be968', '27f09a6a858f',\n",
    "    #    '280e08693c6d', '292a75c0b94e', '29c75c018220', '29d3469bd15d',\n",
    "    #    '2b0a1fa8eba8', '2b8d87addea9', '2cd2340ca14d', '2e9ced2c7976',\n",
    "    #    '2f7504d0f426', '2fbbee1a38e3', '2fc653ca75c7', '31011ade7c0a',\n",
    "    #    '3318a0e3ed6f', '33ceeba8918a', '3452b878e596', '349c5562ee2c',\n",
    "    #    '35826366dfc7', '361366da569e', '3664fe9233f9', '3665c86afaf5',\n",
    "    #    '390b487231ce', '3a9a9dc2cbd9', '3aceb17ef7bd', '3be1545083b7',\n",
    "    #    '3be2f86c3e45', '3c336d6ba566', '3d53bfea61d6', '3df0da2e5966',\n",
    "    #    '405df1b41f9f', '40dce6018935', '416354edd92a', '449766346eb1',\n",
    "    #    '44a41bba1ee7', '44d8c02b369e', '4743bdde25df', '483d6545417f',\n",
    "    #    '4a31811f3558', '4ab54be1a403', '4ac356361be9', '4b45c36f8f5a',\n",
    "    #    '4feda0596965', '519ae2d858b0', '51b23d177971', '51c49c540b4e',\n",
    "    #    '51fdcc8d9fe7', '559ffb7c166a', '55a47ff9dc8a', '55b7f5c99930',\n",
    "    #    '599ca4ed791b', '5aad18e7ce64', '5acc9d63b5fd', '5c088d7e916c',\n",
    "    #    '5c55a5e717d6', '5e816f11f5c3', '5f40907ec171', '5f76965e10cf']\n",
    "    # pred_filesの中から、refに含まれるものだけを抽出する\n",
    "    pred_files = [file for file in pred_files if os.path.basename(file).split(\"_\")[1] in ref]\n",
    "    ref_intersection = [os.path.basename(pred_file).split(\"_\")[1] for pred_file in pred_files]\n",
    "    print(ref_intersection)\n",
    "\n",
    "\n",
    "\n",
    "score, _df = validation_scoring(pred_files, train_events.copy(), pp_params, return_df=True)\n",
    "print(score, _df.shape)\n",
    "\n",
    "\n",
    "# # SIMPLE ENSEMBLE\n",
    "# l_pred_files = [pred_A, pred_B]\n",
    "# score, _df = validation_scoring_ensemble(l_pred_files, train_events.copy(), pp_params, return_df=True) # 0.7593352370275017\n",
    "# print(score, _df.shape)\n",
    "\n",
    "\n",
    "\n",
    "# WBF Ensemble\n",
    "# test_length = 300\n",
    "# score_A, pred_A_df = validation_scoring(pred_A[:test_length], train_events.copy(), pp_params, return_df=True)\n",
    "# score_B, pred_B_df = validation_scoring(pred_B[:test_length], train_events.copy(), pp_params, return_df=True)\n",
    "# score_AB_av = validation_scoring_ensemble([pred_A[:test_length], pred_B[:test_length]], train_events.copy(), pp_params)\n",
    "# pred_AB_wbf_df = weighted_fusion_ennsemble(pred_A_df, pred_B_df, distance_threshold=120)\n",
    "\n",
    "# series_ids = pred_AB_wbf_df['series_id'].unique()\n",
    "# solution =  train_events.loc[train_events['series_id'].isin(series_ids)].copy()\n",
    "# solution = solution[~np.isnan(solution['step'])]\n",
    "# score_AB_wbf = event_detection_ap(solution, pred_AB_wbf_df.copy(), tolerances)\n",
    "\n",
    "# 0.8185768489903773 (20606, 14)0.8202611423370336 (17670, 14)\n",
    "\n",
    "\n",
    "# 0.8176806963633741 (17312, 14)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make dataset for 2nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MEMUSE] memory usage (in before ensemble dataset): 16392.45MB (25.1%)\n",
      "[ensemble dataset] 0.000sec\n",
      "[MEMUSE] memory usage (in after ensemble dataset): 16392.45MB (25.1%)\n",
      "[MEMUSE] memory usage (in before ensemble dataset): 16392.45MB (25.1%)\n",
      "[ensemble dataset] 0.000sec\n",
      "[MEMUSE] memory usage (in after ensemble dataset): 16392.45MB (25.1%)\n",
      "[MEMUSE] memory usage (in before ensemble dataset): 16392.45MB (25.1%)\n",
      "[ensemble dataset] 0.000sec\n",
      "[MEMUSE] memory usage (in after ensemble dataset): 16392.45MB (25.1%)\n",
      "[MEMUSE] memory usage (in before ensemble dataset): 16392.45MB (25.1%)\n",
      "038441c925bb\n",
      "03d92c9f6f8a\n",
      "0402a003dae9\n",
      "04f547b8017d\n",
      "05e1944c3818\n",
      "062cae666e2a\n",
      "062dbd4c95e6\n",
      "08db4255286f\n",
      "0a96f4993bd7\n",
      "0cd1e3d0ed95\n",
      "0ce74d6d2106\n",
      "0cfc06c129cc\n",
      "0d0ad1e77851\n",
      "0dee4fda51c3\n",
      "0ec9fc461819\n",
      "0ef7d94fde99\n",
      "0f572d690310\n",
      "0f9e60a8e56d\n",
      "10469f6765bf\n",
      "1087d7b0ff2e\n",
      "10f8bc1f7b07\n",
      "12d01911d509\n",
      "1319a1935f48\n",
      "137771d19ca2\n",
      "137b99e936ab\n",
      "13b4d6a01d27\n",
      "148471991ffb\n",
      "154fe824ed87\n",
      "16fe2798ed0f\n",
      "1716cd4163b2\n",
      "1762ab70ec76\n",
      "188d4b7cd28b\n",
      "18a0ca03431d\n",
      "18b61dd5aae8\n",
      "1955d568d987\n",
      "1b92be89db4c\n",
      "1c7c0bad1263\n",
      "1d4569cbac0f\n",
      "1e6717d93c1d\n",
      "1f96b9668bdf\n",
      "207eded97727\n",
      "25e2b3dd9c3b\n",
      "2654a87be968\n",
      "27f09a6a858f\n",
      "280e08693c6d\n",
      "292a75c0b94e\n",
      "29c75c018220\n",
      "29d3469bd15d\n",
      "2b0a1fa8eba8\n",
      "2b8d87addea9\n",
      "2cd2340ca14d\n",
      "2e9ced2c7976\n",
      "2f7504d0f426\n",
      "2fbbee1a38e3\n",
      "2fc653ca75c7\n",
      "31011ade7c0a\n",
      "3318a0e3ed6f\n",
      "33ceeba8918a\n",
      "3452b878e596\n",
      "349c5562ee2c\n",
      "35826366dfc7\n",
      "361366da569e\n",
      "3664fe9233f9\n",
      "3665c86afaf5\n",
      "390b487231ce\n",
      "3a9a9dc2cbd9\n",
      "3aceb17ef7bd\n",
      "3be1545083b7\n",
      "3be2f86c3e45\n",
      "3c336d6ba566\n",
      "3d53bfea61d6\n",
      "3df0da2e5966\n",
      "405df1b41f9f\n",
      "40dce6018935\n",
      "416354edd92a\n",
      "449766346eb1\n",
      "44a41bba1ee7\n",
      "44d8c02b369e\n",
      "4743bdde25df\n",
      "483d6545417f\n",
      "4a31811f3558\n",
      "4ab54be1a403\n",
      "4ac356361be9\n",
      "4b45c36f8f5a\n",
      "4feda0596965\n",
      "519ae2d858b0\n",
      "51b23d177971\n",
      "51c49c540b4e\n",
      "51fdcc8d9fe7\n",
      "559ffb7c166a\n",
      "55a47ff9dc8a\n",
      "55b7f5c99930\n",
      "599ca4ed791b\n",
      "5aad18e7ce64\n",
      "5acc9d63b5fd\n",
      "5c088d7e916c\n",
      "5c55a5e717d6\n",
      "5e816f11f5c3\n",
      "5f40907ec171\n",
      "5f76965e10cf\n",
      "5f94bb3e1bed\n",
      "5ffd5e1e81ac\n",
      "601559e1777d\n",
      "60d31b0bec3b\n",
      "60e51cad2ffb\n",
      "612aa8ba44e2\n",
      "653622ac8363\n",
      "655f19eabf1e\n",
      "67f5fc60e494\n",
      "694faf956ebf\n",
      "6a4cd123bd69\n",
      "6bf95a3cf91c\n",
      "6ca4f4fca6a2\n",
      "6d6b9d22d48a\n",
      "6ee4ade1f2bd\n",
      "702bb5387b1e\n",
      "703b5efa9bc1\n",
      "72ba4a8afff4\n",
      "72bbd1ac3edf\n",
      "72d2234e84e4\n",
      "73fb772e50fb\n",
      "7476c0bd18d2\n",
      "7504165f497d\n",
      "752900afe3a6\n",
      "76237b9406d5\n",
      "77ca4db83644\n",
      "7822ee8fe3ec\n",
      "78569a801a38\n",
      "785c9ca4eff7\n",
      "7df249527c63\n",
      "7fd4284b7ee8\n",
      "804594bb1f06\n",
      "808652a666c6\n",
      "83fa182bec3a\n",
      "844f54dcab89\n",
      "854206f602d0\n",
      "87a6cbb7c4ed\n",
      "8877a6586606\n",
      "8898e6db816d\n",
      "89bd631d1769\n",
      "89c7daa72eee\n",
      "8a22387617c3\n",
      "8a306e0890c0\n",
      "8b159a98f485\n",
      "8b8b9e29171c\n",
      "8becc76ea607\n",
      "8e32047cbc1f\n",
      "8f6f15b9f598\n",
      "8fb18e36697d\n",
      "90eac42a9ec9\n",
      "91127c2b0e60\n",
      "91cb6c98201f\n",
      "9277be28a1cf\n",
      "927dd0c35dfd\n",
      "939932f1822d\n",
      "971207c6a525\n",
      "99237ce045e4\n",
      "99b829cbad2d\n",
      "9a340507e36a\n",
      "9aed9ee12ae2\n",
      "9b9cd7b7af8c\n",
      "9c91c546e095\n",
      "9ddd40f2cb36\n",
      "9ee455e4770d\n",
      "9fbdeffbe2ba\n",
      "a167532acca2\n",
      "a261bc4b7470\n",
      "a2b0a64ec9cf\n",
      "a3e59c2ce3f6\n",
      "a4e48102f402\n",
      "a596ad0b82aa\n",
      "a681f9b04b21\n",
      "a81f4472c637\n",
      "a88088855de5\n",
      "a9a2f7fac455\n",
      "a9e5f5314bcb\n",
      "aa81faa78747\n",
      "ad425f3ee76d\n",
      "aed3850f65f0\n",
      "af91d9a50547\n",
      "b1831c4979da\n",
      "b364205aba43\n",
      "b4b75225b224\n",
      "b7188813d58a\n",
      "b737f8c78ec5\n",
      "b750c8c1556c\n",
      "b7fc34995d0f\n",
      "b84960841a75\n",
      "ba8083a2c3b8\n",
      "bb5612895813\n",
      "bccf2f2819f8\n",
      "bdfce9ce62b9\n",
      "bf00506437aa\n",
      "bfa54bd26187\n",
      "bfe41e96d12f\n",
      "c107b5789660\n",
      "c289c8a823e0\n",
      "c3072a759efb\n",
      "c38707ef76df\n",
      "c535634d7dcd\n",
      "c5365a55ebb7\n",
      "c5d08fc3e040\n",
      "c6788e579967\n",
      "c68260cc9e8f\n",
      "c75b4b207bea\n",
      "c7b1283bb7eb\n",
      "c7b2155a4a47\n",
      "c7d693f24684\n",
      "c8053490cec2\n",
      "c908a0ad3e31\n",
      "ca730dbf521d\n",
      "ca732a3c37f7\n",
      "cca14d1966c1\n",
      "ccdee561ee5d\n",
      "ce85771a714c\n",
      "ce9164297046\n",
      "cf13ed7e457a\n",
      "cfeb11428dd7\n",
      "d043c0ca71cd\n",
      "d0f613c700f7\n",
      "d150801f3145\n",
      "d25e479ecbb7\n",
      "d2d6b9af0553\n",
      "d2fef7e4defd\n",
      "d3dddd3c0e00\n",
      "d515236bdeec\n",
      "d5be621fd9aa\n",
      "d5e47b94477e\n",
      "d8de352c2657\n",
      "d93b0c7de16b\n",
      "d9e887091a5c\n",
      "dacc6d652e35\n",
      "db5e0ee1c0ab\n",
      "db75092f0530\n",
      "dc80ca623d71\n",
      "de6fedfb6139\n",
      "def21f50dd3c\n",
      "df33ae359fb5\n",
      "dfc3ccebfdc9\n",
      "dff367373725\n",
      "e0686434d029\n",
      "e0d7b0dcf9f3\n",
      "e11b9d69f856\n",
      "e1f2a4f991cb\n",
      "e1f5abb82285\n",
      "e2a849d283c0\n",
      "e2b60820c325\n",
      "e30cb792a2bc\n",
      "e34b496b84ce\n",
      "e4500e7e19e1\n",
      "e586cbfa7762\n",
      "e69aff66e0cb\n",
      "e6ddbaaf0639\n",
      "e867b5133665\n",
      "e8d0a37c3eba\n",
      "ea0770830757\n",
      "ebb6fae8ed43\n",
      "ebd76e93ec7d\n",
      "ece2561f07e9\n",
      "ee4e0e3afd3d\n",
      "eec197a4bdca\n",
      "eef041dd50aa\n",
      "efbfc4526d58\n",
      "f0482490923c\n",
      "f2c2436cf7b7\n",
      "f564985ab692\n",
      "f56824b503a0\n",
      "f6d2cc003183\n",
      "f7eb179216c2\n",
      "f88e18cb4100\n",
      "f8a8da8bdd00\n",
      "f981a0805fd0\n",
      "fa149c3c4bde\n",
      "fb223ed2278c\n",
      "fbf33b1a2c10\n",
      "fcca183903b7\n",
      "fe90110788d2\n",
      "[ensemble dataset] 125.037sec\n",
      "[MEMUSE] memory usage (in after ensemble dataset): 16341.94MB (25.0%)\n"
     ]
    }
   ],
   "source": [
    "pp_params = {'awake_threshold': 0.01118579240197301, 'sleep_threshold': 0.9741809981418406, 'awake_sleep_diff_threshold': 0.0036734165080717115, \n",
    "            'peak_kernel_size': 4.689964359639259, 'peak_threshold': 0.05135826117540523, \n",
    "            'event_before_length': 68.38372582053368, 'event_after_length': 99.69200455879107, 'prior_conf_dev': 0.29102092096947463,\n",
    "            \"averaging_weight\": [0.333, 0.333, 0.333, 0.00, 0, 0]}\n",
    "\n",
    "num_folds = 2 if Cfg.IS_DEBUG==\"True\" else 5\n",
    "pred_files_A0 = []\n",
    "for i in range(5):\n",
    "    pred_files_A0 += glob.glob(os.path.join(Cfg.weight_dir_1dcnn, f\"exp00_run_00_SplitStem{num_folds}foldSEED42controledStride_fold{i}/pred090/*.parquet\"))\n",
    "\n",
    "pred_files_A1 = []\n",
    "for i in range(5):\n",
    "    pred_files_A1 += glob.glob(os.path.join(Cfg.weight_dir_1dcnn, f\"exp00_run_00_SplitStem{num_folds}foldSEED111controledStride_fold{i}/pred090/*.parquet\")) # \n",
    "\n",
    "pred_files_B0 = []\n",
    "for i in range(5):\n",
    "    pred_files_B0 += glob.glob(os.path.join(Cfg.weight_dir_1dcnn, f\"exp00_run_01_SplitStem{num_folds}foldSEED42normal_fold{i}/pred090/*.parquet\"))\n",
    "\n",
    "pred_files_B1 = []\n",
    "for i in range(5):\n",
    "    pred_files_B1 += glob.glob(os.path.join(Cfg.weight_dir_1dcnn, f\"exp00_run_01_SplitStem{num_folds}foldSEED111normal_fold{i}/pred090/*.parquet\"))\n",
    "\n",
    "# sort\n",
    "pred_A0_base = [os.path.basename(f) for f in pred_files_A0]\n",
    "pred_A1_base = [os.path.basename(f) for f in pred_files_A1]\n",
    "pred_B0_base = [os.path.basename(f) for f in pred_files_B0]\n",
    "pred_B1_base = [os.path.basename(f) for f in pred_files_B1]\n",
    "\n",
    "pred_files_A0 = [pred_files_A0[i] for i in np.argsort(pred_A0_base)]\n",
    "pred_files_A1 = [pred_files_A1[i] for i in np.argsort(pred_A1_base)]\n",
    "pred_files_B0 = [pred_files_B0[i] for i in np.argsort(pred_B0_base)]\n",
    "pred_files_B1 = [pred_files_B1[i] for i in np.argsort(pred_B1_base)]\n",
    "\n",
    "print(\"---make dataset for second model---\")\n",
    "\n",
    "with timer(\"ensemble dataset\"):\n",
    "    ensemble_files =  [pred_files_A0, pred_files_A1]\n",
    "    make_dataset_for_second_model_ensemble(ensemble_files, train_events, pp_params, save_path=os.path.join(Cfg.preprocess_dir, \"df_second_model_0.feather\"))\n",
    "\n",
    "with timer(\"ensemble dataset\"):\n",
    "    ensemble_files =  [pred_files_B0, pred_files_B1]\n",
    "    make_dataset_for_second_model_ensemble(ensemble_files, train_events, pp_params, save_path=os.path.join(Cfg.preprocess_dir, \"df_second_model_1.feather\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
